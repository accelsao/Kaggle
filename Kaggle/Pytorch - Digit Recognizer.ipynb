{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.utils as vutils\n\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\nprint(os.listdir(\"../input\"))\nprint('cuda: {}, device: {}'.format(use_cuda, device))\nprint('pytorch version: {}'.format(torch.__version__))","execution_count":1,"outputs":[{"output_type":"stream","text":"['train.csv', 'sample_submission.csv', 'test.csv']\ncuda: True, device: cuda\npytorch version: 1.0.1.post2\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/train.csv')\ntest_df = pd.read_csv('../input/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MNIST(Dataset):\n    def __init__(self, file_path, transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(mean=(0.5,), std=(0.5,))\n    ])):\n        df = pd.read_csv(file_path)\n        if df.shape[1] == 784:\n            # test data\n            # self.X = df.values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.X = df.values.reshape(-1, 28, 28, 1).astype(np.uint8)\n            self.y = None\n        else:\n            # training data\n            # self.X = df.iloc[:, 1:].values.reshape((-1, 28, 28)).astype(np.uint8)[:, :, :, None]\n            self.X = df.iloc[:, 1:].values.reshape(-1, 28, 28, 1).astype(np.uint8)\n            self.y = torch.from_numpy(df.iloc[:, 0].values)\n\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.X)\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return self.transform(self.X[idx]), self.y[idx]\n        else:\n            return self.transform(self.X[idx])","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\ntrain_dataset = MNIST('../input/train.csv', transform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(degrees=20),\n    transforms.RandomAffine(3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,))\n]))\ntest_dataset = MNIST('../input/test.csv')\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = next(iter(train_loader))\nplt.figure(figsize=(4,4))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(vutils.make_grid(img[0].to(device)[:1], padding=0, normalize=True).cpu(),(1,2,0)))","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7f3435c78748>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 288x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Dropout(p=0.25),\n        )\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.25),\n            nn.Linear(128 * 7 * 7, 512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p=0.25),\n            nn.Linear(512, 10),\n        )\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n    ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Net()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\ncriterion = nn.CrossEntropyLoss()\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\nif use_cuda:\n    model = model.to(device)\n    criterion = criterion.to(device)\nprint(model)","execution_count":9,"outputs":[{"output_type":"stream","text":"Net(\n  (features): Sequential(\n    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU(inplace)\n    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (4): ReLU(inplace)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Dropout(p=0.25)\n    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (9): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (14): ReLU(inplace)\n    (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (16): Dropout(p=0.25)\n  )\n  (classifier): Sequential(\n    (0): Dropout(p=0.25)\n    (1): Linear(in_features=6272, out_features=512, bias=True)\n    (2): ReLU(inplace)\n    (3): Dropout(p=0.25)\n    (4): Linear(in_features=512, out_features=10, bias=True)\n  )\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(model, device, train_loader, optimizer, epoch):\n    model.train()\n    lr_scheduler.step()\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        if batch_idx % 100 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_epochs = 20\nfor epoch in range(1, n_epochs + 1):\n    train(model, device, train_loader, optimizer, epoch)\n    test(model, device, train_loader)","execution_count":12,"outputs":[{"output_type":"stream","text":"Train Epoch: 1 [0/42000 (0%)]\tLoss: 2.335302\nTrain Epoch: 1 [6400/42000 (15%)]\tLoss: 0.543683\nTrain Epoch: 1 [12800/42000 (30%)]\tLoss: 0.210248\nTrain Epoch: 1 [19200/42000 (46%)]\tLoss: 0.171113\nTrain Epoch: 1 [25600/42000 (61%)]\tLoss: 0.144800\nTrain Epoch: 1 [32000/42000 (76%)]\tLoss: 0.086634\nTrain Epoch: 1 [38400/42000 (91%)]\tLoss: 0.120456\n\nTest set: Average loss: 0.1272, Accuracy: 40417/42000 (96%)\n\nTrain Epoch: 2 [0/42000 (0%)]\tLoss: 0.261529\nTrain Epoch: 2 [6400/42000 (15%)]\tLoss: 0.191548\nTrain Epoch: 2 [12800/42000 (30%)]\tLoss: 0.230548\nTrain Epoch: 2 [19200/42000 (46%)]\tLoss: 0.091214\nTrain Epoch: 2 [25600/42000 (61%)]\tLoss: 0.179297\nTrain Epoch: 2 [32000/42000 (76%)]\tLoss: 0.109263\nTrain Epoch: 2 [38400/42000 (91%)]\tLoss: 0.229464\n\nTest set: Average loss: 0.0766, Accuracy: 40985/42000 (98%)\n\nTrain Epoch: 3 [0/42000 (0%)]\tLoss: 0.187596\nTrain Epoch: 3 [6400/42000 (15%)]\tLoss: 0.089422\nTrain Epoch: 3 [12800/42000 (30%)]\tLoss: 0.174470\nTrain Epoch: 3 [19200/42000 (46%)]\tLoss: 0.094605\nTrain Epoch: 3 [25600/42000 (61%)]\tLoss: 0.149180\nTrain Epoch: 3 [32000/42000 (76%)]\tLoss: 0.115229\nTrain Epoch: 3 [38400/42000 (91%)]\tLoss: 0.212086\n\nTest set: Average loss: 0.0663, Accuracy: 41150/42000 (98%)\n\nTrain Epoch: 4 [0/42000 (0%)]\tLoss: 0.244545\nTrain Epoch: 4 [6400/42000 (15%)]\tLoss: 0.032635\nTrain Epoch: 4 [12800/42000 (30%)]\tLoss: 0.247479\nTrain Epoch: 4 [19200/42000 (46%)]\tLoss: 0.152888\nTrain Epoch: 4 [25600/42000 (61%)]\tLoss: 0.253205\nTrain Epoch: 4 [32000/42000 (76%)]\tLoss: 0.055719\nTrain Epoch: 4 [38400/42000 (91%)]\tLoss: 0.061936\n\nTest set: Average loss: 0.0589, Accuracy: 41253/42000 (98%)\n\nTrain Epoch: 5 [0/42000 (0%)]\tLoss: 0.116214\nTrain Epoch: 5 [6400/42000 (15%)]\tLoss: 0.138926\nTrain Epoch: 5 [12800/42000 (30%)]\tLoss: 0.114807\nTrain Epoch: 5 [19200/42000 (46%)]\tLoss: 0.056055\nTrain Epoch: 5 [25600/42000 (61%)]\tLoss: 0.134381\nTrain Epoch: 5 [32000/42000 (76%)]\tLoss: 0.146669\nTrain Epoch: 5 [38400/42000 (91%)]\tLoss: 0.033003\n\nTest set: Average loss: 0.0647, Accuracy: 41204/42000 (98%)\n\nTrain Epoch: 6 [0/42000 (0%)]\tLoss: 0.145540\nTrain Epoch: 6 [6400/42000 (15%)]\tLoss: 0.171767\nTrain Epoch: 6 [12800/42000 (30%)]\tLoss: 0.107132\nTrain Epoch: 6 [19200/42000 (46%)]\tLoss: 0.063729\nTrain Epoch: 6 [25600/42000 (61%)]\tLoss: 0.032643\nTrain Epoch: 6 [32000/42000 (76%)]\tLoss: 0.056508\nTrain Epoch: 6 [38400/42000 (91%)]\tLoss: 0.050744\n\nTest set: Average loss: 0.0499, Accuracy: 41349/42000 (98%)\n\nTrain Epoch: 7 [0/42000 (0%)]\tLoss: 0.057285\nTrain Epoch: 7 [6400/42000 (15%)]\tLoss: 0.196234\nTrain Epoch: 7 [12800/42000 (30%)]\tLoss: 0.050437\nTrain Epoch: 7 [19200/42000 (46%)]\tLoss: 0.147712\nTrain Epoch: 7 [25600/42000 (61%)]\tLoss: 0.024627\nTrain Epoch: 7 [32000/42000 (76%)]\tLoss: 0.097636\nTrain Epoch: 7 [38400/42000 (91%)]\tLoss: 0.050631\n\nTest set: Average loss: 0.0416, Accuracy: 41463/42000 (99%)\n\nTrain Epoch: 8 [0/42000 (0%)]\tLoss: 0.027334\nTrain Epoch: 8 [6400/42000 (15%)]\tLoss: 0.059041\nTrain Epoch: 8 [12800/42000 (30%)]\tLoss: 0.024977\nTrain Epoch: 8 [19200/42000 (46%)]\tLoss: 0.031329\nTrain Epoch: 8 [25600/42000 (61%)]\tLoss: 0.134264\nTrain Epoch: 8 [32000/42000 (76%)]\tLoss: 0.038356\nTrain Epoch: 8 [38400/42000 (91%)]\tLoss: 0.003665\n\nTest set: Average loss: 0.0291, Accuracy: 41632/42000 (99%)\n\nTrain Epoch: 9 [0/42000 (0%)]\tLoss: 0.112561\nTrain Epoch: 9 [6400/42000 (15%)]\tLoss: 0.013820\nTrain Epoch: 9 [12800/42000 (30%)]\tLoss: 0.151526\nTrain Epoch: 9 [19200/42000 (46%)]\tLoss: 0.021773\nTrain Epoch: 9 [25600/42000 (61%)]\tLoss: 0.035298\nTrain Epoch: 9 [32000/42000 (76%)]\tLoss: 0.107755\nTrain Epoch: 9 [38400/42000 (91%)]\tLoss: 0.012190\n\nTest set: Average loss: 0.0255, Accuracy: 41684/42000 (99%)\n\nTrain Epoch: 10 [0/42000 (0%)]\tLoss: 0.006521\nTrain Epoch: 10 [6400/42000 (15%)]\tLoss: 0.004674\nTrain Epoch: 10 [12800/42000 (30%)]\tLoss: 0.019295\nTrain Epoch: 10 [19200/42000 (46%)]\tLoss: 0.006692\nTrain Epoch: 10 [25600/42000 (61%)]\tLoss: 0.001105\nTrain Epoch: 10 [32000/42000 (76%)]\tLoss: 0.080371\nTrain Epoch: 10 [38400/42000 (91%)]\tLoss: 0.017226\n\nTest set: Average loss: 0.0246, Accuracy: 41690/42000 (99%)\n\nTrain Epoch: 11 [0/42000 (0%)]\tLoss: 0.011894\nTrain Epoch: 11 [6400/42000 (15%)]\tLoss: 0.288432\nTrain Epoch: 11 [12800/42000 (30%)]\tLoss: 0.029920\nTrain Epoch: 11 [19200/42000 (46%)]\tLoss: 0.005488\nTrain Epoch: 11 [25600/42000 (61%)]\tLoss: 0.018979\nTrain Epoch: 11 [32000/42000 (76%)]\tLoss: 0.058230\nTrain Epoch: 11 [38400/42000 (91%)]\tLoss: 0.106296\n\nTest set: Average loss: 0.0243, Accuracy: 41696/42000 (99%)\n\nTrain Epoch: 12 [0/42000 (0%)]\tLoss: 0.004587\nTrain Epoch: 12 [6400/42000 (15%)]\tLoss: 0.004760\nTrain Epoch: 12 [12800/42000 (30%)]\tLoss: 0.036457\nTrain Epoch: 12 [19200/42000 (46%)]\tLoss: 0.003790\nTrain Epoch: 12 [25600/42000 (61%)]\tLoss: 0.022689\nTrain Epoch: 12 [32000/42000 (76%)]\tLoss: 0.006468\nTrain Epoch: 12 [38400/42000 (91%)]\tLoss: 0.013172\n\nTest set: Average loss: 0.0227, Accuracy: 41700/42000 (99%)\n\nTrain Epoch: 13 [0/42000 (0%)]\tLoss: 0.066829\nTrain Epoch: 13 [6400/42000 (15%)]\tLoss: 0.062904\nTrain Epoch: 13 [12800/42000 (30%)]\tLoss: 0.200438\nTrain Epoch: 13 [19200/42000 (46%)]\tLoss: 0.044041\nTrain Epoch: 13 [25600/42000 (61%)]\tLoss: 0.065954\nTrain Epoch: 13 [32000/42000 (76%)]\tLoss: 0.059566\nTrain Epoch: 13 [38400/42000 (91%)]\tLoss: 0.019086\n\nTest set: Average loss: 0.0208, Accuracy: 41730/42000 (99%)\n\nTrain Epoch: 14 [0/42000 (0%)]\tLoss: 0.041260\nTrain Epoch: 14 [6400/42000 (15%)]\tLoss: 0.029407\nTrain Epoch: 14 [12800/42000 (30%)]\tLoss: 0.105610\nTrain Epoch: 14 [19200/42000 (46%)]\tLoss: 0.012503\nTrain Epoch: 14 [25600/42000 (61%)]\tLoss: 0.008912\nTrain Epoch: 14 [32000/42000 (76%)]\tLoss: 0.048077\nTrain Epoch: 14 [38400/42000 (91%)]\tLoss: 0.070111\n\nTest set: Average loss: 0.0210, Accuracy: 41729/42000 (99%)\n\nTrain Epoch: 15 [0/42000 (0%)]\tLoss: 0.002493\nTrain Epoch: 15 [6400/42000 (15%)]\tLoss: 0.015871\nTrain Epoch: 15 [12800/42000 (30%)]\tLoss: 0.007826\nTrain Epoch: 15 [19200/42000 (46%)]\tLoss: 0.003347\nTrain Epoch: 15 [25600/42000 (61%)]\tLoss: 0.020753\nTrain Epoch: 15 [32000/42000 (76%)]\tLoss: 0.004982\nTrain Epoch: 15 [38400/42000 (91%)]\tLoss: 0.014306\n\nTest set: Average loss: 0.0205, Accuracy: 41723/42000 (99%)\n\nTrain Epoch: 16 [0/42000 (0%)]\tLoss: 0.037084\nTrain Epoch: 16 [6400/42000 (15%)]\tLoss: 0.009920\nTrain Epoch: 16 [12800/42000 (30%)]\tLoss: 0.029767\nTrain Epoch: 16 [19200/42000 (46%)]\tLoss: 0.016937\nTrain Epoch: 16 [25600/42000 (61%)]\tLoss: 0.005726\nTrain Epoch: 16 [32000/42000 (76%)]\tLoss: 0.002443\nTrain Epoch: 16 [38400/42000 (91%)]\tLoss: 0.026159\n\nTest set: Average loss: 0.0204, Accuracy: 41740/42000 (99%)\n\nTrain Epoch: 17 [0/42000 (0%)]\tLoss: 0.059258\nTrain Epoch: 17 [6400/42000 (15%)]\tLoss: 0.015672\nTrain Epoch: 17 [12800/42000 (30%)]\tLoss: 0.009998\nTrain Epoch: 17 [19200/42000 (46%)]\tLoss: 0.021581\nTrain Epoch: 17 [25600/42000 (61%)]\tLoss: 0.049161\nTrain Epoch: 17 [32000/42000 (76%)]\tLoss: 0.106600\nTrain Epoch: 17 [38400/42000 (91%)]\tLoss: 0.011841\n\nTest set: Average loss: 0.0193, Accuracy: 41751/42000 (99%)\n\nTrain Epoch: 18 [0/42000 (0%)]\tLoss: 0.212998\nTrain Epoch: 18 [6400/42000 (15%)]\tLoss: 0.017732\nTrain Epoch: 18 [12800/42000 (30%)]\tLoss: 0.001547\nTrain Epoch: 18 [19200/42000 (46%)]\tLoss: 0.003683\nTrain Epoch: 18 [25600/42000 (61%)]\tLoss: 0.004206\nTrain Epoch: 18 [32000/42000 (76%)]\tLoss: 0.041695\nTrain Epoch: 18 [38400/42000 (91%)]\tLoss: 0.067126\n\nTest set: Average loss: 0.0199, Accuracy: 41734/42000 (99%)\n\nTrain Epoch: 19 [0/42000 (0%)]\tLoss: 0.001733\nTrain Epoch: 19 [6400/42000 (15%)]\tLoss: 0.106858\nTrain Epoch: 19 [12800/42000 (30%)]\tLoss: 0.051554\nTrain Epoch: 19 [19200/42000 (46%)]\tLoss: 0.052348\nTrain Epoch: 19 [25600/42000 (61%)]\tLoss: 0.101663\nTrain Epoch: 19 [32000/42000 (76%)]\tLoss: 0.014346\nTrain Epoch: 19 [38400/42000 (91%)]\tLoss: 0.001986\n\nTest set: Average loss: 0.0201, Accuracy: 41731/42000 (99%)\n\nTrain Epoch: 20 [0/42000 (0%)]\tLoss: 0.009335\nTrain Epoch: 20 [6400/42000 (15%)]\tLoss: 0.027236\nTrain Epoch: 20 [12800/42000 (30%)]\tLoss: 0.012425\nTrain Epoch: 20 [19200/42000 (46%)]\tLoss: 0.002194\nTrain Epoch: 20 [25600/42000 (61%)]\tLoss: 0.014712\nTrain Epoch: 20 [32000/42000 (76%)]\tLoss: 0.044395\nTrain Epoch: 20 [38400/42000 (91%)]\tLoss: 0.021447\n\nTest set: Average loss: 0.0195, Accuracy: 41751/42000 (99%)\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prediciton(test_loader):\n    model.eval()\n    test_pred = torch.LongTensor()\n    with torch.no_grad():\n        for data in test_loader:\n            data = data.to(device)\n            output = model(data)\n            pred = output.argmax(dim=1, keepdim=True).cpu()\n            test_pred = torch.cat((test_pred, pred), dim=0)\n    return test_pred","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = prediciton(test_loader)\nout_df = pd.DataFrame(np.c_[np.arange(1, len(test_dataset)+1)[:,None], test_pred.numpy()], \n                      columns=['ImageId', 'Label'])","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(out_df)","execution_count":21,"outputs":[{"output_type":"stream","text":"       ImageId  Label\n0            1      2\n1            2      0\n2            3      9\n3            4      9\n4            5      3\n5            6      7\n6            7      0\n7            8      3\n8            9      0\n9           10      3\n10          11      5\n11          12      7\n12          13      4\n13          14      0\n14          15      4\n15          16      3\n16          17      3\n17          18      1\n18          19      9\n19          20      0\n20          21      9\n21          22      1\n22          23      1\n23          24      5\n24          25      7\n25          26      4\n26          27      2\n27          28      7\n28          29      4\n29          30      7\n...        ...    ...\n27970    27971      5\n27971    27972      0\n27972    27973      4\n27973    27974      8\n27974    27975      0\n27975    27976      3\n27976    27977      6\n27977    27978      0\n27978    27979      1\n27979    27980      9\n27980    27981      3\n27981    27982      1\n27982    27983      1\n27983    27984      0\n27984    27985      4\n27985    27986      5\n27986    27987      2\n27987    27988      2\n27988    27989      9\n27989    27990      6\n27990    27991      7\n27991    27992      6\n27992    27993      1\n27993    27994      9\n27994    27995      7\n27995    27996      9\n27996    27997      7\n27997    27998      3\n27998    27999      9\n27999    28000      2\n\n[28000 rows x 2 columns]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"out_df.to_csv('submission.csv', index=False)","execution_count":24,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}