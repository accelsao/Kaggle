{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class NonLocalBlockNd(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True, f_type='embedded_gaussian'):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert dimension in [1,2,3]\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "        \n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "        \n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
    "            bn = nn.BatchNorm1d\n",
    "            \n",
    "        self.g = conv_nd(self.in_channels, self.inter_channels, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(self.inter_channels, self.in_channels, kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight,0)\n",
    "            nn.init.constant_(self.W[1].bias,0)\n",
    "        else:\n",
    "            self.W = conv_nd(self.inter_channels, self.in_channels, kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "            \n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "            \n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)    \n",
    "        \n",
    "        self.concat_project = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.inter_channels * 2, out_channels=1, kernel_size=1, stride=1, padding=0, bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "            \n",
    "            \n",
    "        if f_type == 'gaussian':\n",
    "            self.theta = lambda z: z\n",
    "            self.phi = max_pool_layer if sub_sample else lambda z: z\n",
    "        \n",
    "        \n",
    "        self.f_type = f_type\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Shape:\n",
    "            x : (batch_size, channels, t, h, w)\n",
    "        '''\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1) # b,c,t*h*w\n",
    "        g_x = g_x.permute(0, 2, 1) # b,t*h*w, c\n",
    "\n",
    "\n",
    "        \n",
    "        if self.f_type == 'concatenation':\n",
    "            # (b,c,,N,1)\n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1, 1)\n",
    "            # (b,c,1,N)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, 1, -1)\n",
    "            h = theta_x.size(2)\n",
    "            w = phi_x.size(3)\n",
    "            theta_x = theta_x.repeat(1, 1, 1, w)\n",
    "            phi_x = phi_x.repeat(1, 1, h, 1)\n",
    "            concat_feature = torch.cat([theta_x, phi_x], dim=1)\n",
    "            f = self.concat_project(concat_feature)\n",
    "            b, _, h, w = f.size()\n",
    "            f = f.view(b, h, w)\n",
    "        else:\n",
    "            \n",
    "            theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "            theta_x = theta_x.permute(0, 2, 1)\n",
    "            phi_x = self.phi(x).view(batch_size, self.inter_channels, -1) # b,c,t*h*w\n",
    "            f = torch.matmul(theta_x, phi_x)\n",
    "            \n",
    "        \n",
    "        if self.f_type in ('embedded_gaussian','gaussian'):\n",
    "            f_C = F.softmax(f, dim=-1)\n",
    "        elif self.f_type in ('dot_product','concatenation'):\n",
    "            N = f.size(-1)\n",
    "            f_C = f / N\n",
    "        \n",
    "\n",
    "        y = torch.matmul(f_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous() # contiguous() make new tensor\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "        return z            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n",
      "torch.Size([32, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "for (sub_sample, bn_layer) in [(True, True), (False, False), (True, False), (False, True)]:\n",
    "    img = torch.zeros(32, 3, 32, 32)\n",
    "    net = NonLocalBlockNd(3, dimension=2, sub_sample=sub_sample, bn_layer=bn_layer, f_type='concatenation')\n",
    "    out = net(img)\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/1wjadJQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3,2,5,1))\n",
    "print(x)\n",
    "z = x.repeat((1,1,1,5))\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
